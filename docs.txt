=== ./docs/examples.md ===
# Bardo Examples

This document provides detailed information on the example applications and benchmarks included with Bardo.

## Benchmarks

### Double Pole Balancing (DPB)

A classic control problem where a neural network learns to balance two poles of different lengths on a cart.

#### Overview

In this problem, we try to balance two poles of different lengths simultaneously on a cart. The cart can move left and right along a track, and the neural network must apply the correct forces to keep both poles balanced.

- One pole is 0.1 meters long
- The other pole is 1.0 meter long 
- The closer the lengths of the two poles are, the more difficult the problem becomes

#### Running the Example

```elixir
# Run with default settings
Bardo.Examples.Benchmarks.Dpb.run()

# Run with custom settings
Bardo.Examples.Benchmarks.Dpb.run(%{
  population_size: 100,
  max_generations: 200,
  use_damping: true  # Enables damping to discourage rapid cart movements
})
```

#### What to Expect

- With standard settings, a solution typically emerges after ~2,300 evaluations
- The neural network learns to make small, precise movements to balance both poles
- You can observe the evolution progress through the console output or visualizations

#### Implementation Details

The DPB example provides two variants:
- Without damping: Allows fast cart movements as long as poles stay balanced
- With damping: Penalizes high velocity and rapid changes, encouraging smoother control

The neural network receives these inputs:
- Cart position and velocity 
- First pole angle and angular velocity
- Second pole angle and angular velocity

It produces a single output:
- Force value (in Newtons) to apply to the cart, saturated at 10N magnitude

## Applications

### Flatland (Predator vs Prey Simulation)

A more complex simulation where predator and prey agents co-evolve in a 2D world.

#### Overview

Flatland creates a simulated 2D environment where:
- Predator agents (red) try to catch and consume prey agents (blue)
- Prey agents try to survive by avoiding predators and consuming plants (green)
- Both species evolve more sophisticated strategies over time

This example demonstrates:
- Co-evolution of competing species
- Complex emergent behaviors
- Steady-state evolution (ongoing birth and death rather than distinct generations)

#### Running the Example

```elixir
# Run with default settings
Bardo.Examples.Applications.Flatland.run()

# Run with custom settings
Bardo.Examples.Applications.Flatland.run(%{
  predator_population_size: 10,
  prey_population_size: 20,
  plant_quantity: 40,
  max_evaluations: 10000
})
```

#### What to Expect

You'll observe fascinating co-evolutionary dynamics:
- Initially random behavior becomes increasingly strategic
- Predators may evolve trapping or ambush tactics
- Prey develop evasion strategies and efficient foraging
- The population dynamics reach different equilibria depending on which species evolves effective strategies first

#### Implementation Details

Each agent has:
- Distance scanners (sensors that detect objects at different angles)
- Color scanners (sensors that identify object types by color)
- Two-wheel drive actuators (for movement control)

The evolutionary progress can be tracked through:
- Average fitness over time
- Neural network complexity
- Population diversity
- Population turnover (death rates)

## Custom Example Development

To create your own example using Bardo:

1. Define your environment and interaction rules
2. Create custom sensors and actuators
3. Set up the evolution parameters
4. Configure fitness functions

See the [advanced guide](advanced.md) for detailed instructions on developing custom examples.
=== ./docs/algo_trading_guide.md ===
# Algorithmic Trading with Bardo

This guide explains how to use Bardo's algorithmic trading capabilities to develop, train, and deploy neural network-based trading strategies.

## Table of Contents

1. [Introduction](#introduction)
2. [Architecture Overview](#architecture-overview)
3. [Substrate Encoding](#substrate-encoding)
4. [Distributed Training](#distributed-training)
5. [Live Trading](#live-trading)
6. [Continuous Learning](#continuous-learning)
7. [Example Workflows](#example-workflows)
8. [Advanced Topics](#advanced-topics)

## Introduction

Bardo's algorithmic trading module provides a complete framework for developing neural network-based trading strategies using neuroevolution. This approach allows you to:

1. Evolve neural networks that can process market data and make trading decisions
2. Use advanced encoding techniques (like substrate encoding) for better pattern recognition
3. Distribute training across multiple nodes for faster results
4. Deploy trained agents to live markets with risk management
5. Implement continuous learning for adaptive agents

The system is highly modular and extensible, allowing you to customize each component for your specific needs.

## Architecture Overview

The algorithmic trading module consists of several key components:

- **Market Simulators**: Simulate market environments for training
- **Broker Interfaces**: Connect to external trading platforms
- **Neural Network Encoding**: Methods for representing market data to neural networks
- **Distributed Training**: Tools for parallel evolution across multiple nodes
- **Live Agents**: Deployable agents that can trade real markets
- **Performance Analytics**: Tools for evaluating trading strategies

These components work together to create a complete pipeline from strategy development to live trading:

```
Market Data → Neural Network → Trading Decisions → Execution → Performance Evaluation → Evolution
```

## Substrate Encoding

Substrate encoding is a powerful technique for representing market data in a geometric space that neural networks can more easily process. It provides several advantages over traditional vector-based inputs:

- Better pattern recognition
- More efficient representation of complex data
- Improved generalization to new market conditions
- Natural regularization of network structure

### How Substrate Encoding Works

Substrate encoding maps market data into a 3D coordinate space:

- X-axis: Time (from recent to older candles)
- Y-axis: Price levels (from high to low)
- Z-axis: Data types (OHLC, volume, indicators)

Neurons are then placed at specific coordinates in this space, and connections are established based on geometric rules and evolved weights.

### Using Substrate Encoding

To use substrate encoding in your trading strategy:

```elixir
# Create a substrate-encoded genotype
genotype = Bardo.Examples.Applications.AlgoTrading.SubstrateEncoding.create_substrate_genotype(%{
  input_time_points: 60,    # 60 time periods of data
  input_price_levels: 20,   # 20 price levels
  input_data_types: 10,     # 10 different data types
  hidden_layers: 2,         # 2 hidden layers
  hidden_neurons_per_layer: 20, # 20 neurons per hidden layer
  output_neurons: 3         # 3 outputs (direction, size, risk)
})

# Convert market data to substrate representation
grid = Bardo.Examples.Applications.AlgoTrading.SubstrateEncoding.convert_price_data_to_substrate(
  price_data,    # List of price candles
  indicators,    # Map of technical indicators
  60, 20, 10     # Dimensions matching the genotype
)

# Flatten to neuron inputs
inputs = Bardo.Examples.Applications.AlgoTrading.SubstrateEncoding.flatten_substrate_grid(
  grid, genotype
)

# Activate the neural network with the inputs
{:ok, outputs} = Bardo.AgentManager.Cortex.activate(cortex, inputs)
```

## Distributed Training

Distributed training allows you to leverage multiple machines or nodes to speed up the evolutionary process. This is especially valuable for algorithmic trading, where the search space is large and evaluation can be computationally intensive.

### Key Concepts

1. **Island-based Evolution**: Population is divided into "islands" that evolve independently
2. **Migration**: Periodically, the best individuals migrate between islands
3. **Node Distribution**: Islands can be distributed across multiple physical or virtual machines
4. **Fault Tolerance**: The system can recover from node failures by migrating islands

### Setting Up Distributed Training

To set up distributed training:

1. Connect the nodes (this is a standard Erlang distribution setup)
2. Define your experiment configuration
3. Start distributed training

```elixir
# Start distributed training
{:ok, experiment_id} = Bardo.Examples.Applications.AlgoTrading.DistributedTraining.start_distributed_training(
  :my_trading_experiment,  # Unique experiment ID
  %{                       # Configuration options
    market: :forex,
    symbol: "EURUSD",
    timeframe: 15,
    population_size: 500,  # Will be divided across islands
    generations: 200,
    mutation_rate: 0.1,
    elite_fraction: 0.1,
    tournament_size: 5
  },
  [
    nodes: [:node1@host, :node2@host, :node3@host],  # Nodes to use
    islands: 6,            # Number of islands (population subgroups)
    migration_interval: 10, # How often to migrate individuals between islands
    migration_rate: 0.1     # Percentage of population to migrate
  ]
)

# Check training status
status = Bardo.Examples.Applications.AlgoTrading.DistributedTraining.get_training_status(experiment_id)

# Get the best agent when training is complete
{:ok, best_agent} = Bardo.Examples.Applications.AlgoTrading.DistributedTraining.get_best_agent(experiment_id)

# Stop training early if needed
Bardo.Examples.Applications.AlgoTrading.DistributedTraining.stop_distributed_training(experiment_id)
```

## Live Trading

Once you have trained a successful trading strategy, you can deploy it to live markets using the LiveAgent module.

### Setting Up a Live Agent

```elixir
# Start a live trading agent
{:ok, agent_id} = Bardo.Examples.Applications.AlgoTrading.LiveAgent.start_link(
  :my_trading_agent,    # Agent ID
  best_agent,           # Trained neural network genotype
  Bardo.Examples.Applications.AlgoTrading.Brokers.MetaTrader,  # Broker module
  %{                    # Broker configuration
    symbol: "EURUSD",
    timeframe: 15,
    account_id: "12345678",
    api_key: "your_api_key",
    api_url: "http://localhost:5000"
  },
  [                    # Agent options
    risk_params: %{
      risk_per_trade: 0.01,  # 1% risk per trade
      max_drawdown: 0.10,    # 10% maximum drawdown
      stop_loss: 0.02,       # 2% stop loss
      take_profit: 0.04      # 4% take profit
    },
    substrate_encoding: true,  # Use substrate encoding
    adaptation_enabled: false  # Start without continuous learning
  ]
)

# Get agent status
status = Bardo.Examples.Applications.AlgoTrading.LiveAgent.get_status(agent_id)

# Update risk parameters
Bardo.Examples.Applications.AlgoTrading.LiveAgent.update_risk_params(agent_id, %{
  risk_per_trade: 0.005,  # Reduce risk to 0.5%
  max_drawdown: 0.05      # Reduce max drawdown to 5%
})

# Close all positions and stop the agent
Bardo.Examples.Applications.AlgoTrading.LiveAgent.close_all_positions(agent_id)
Bardo.Examples.Applications.AlgoTrading.LiveAgent.stop_agent(agent_id)
```

### Deploying Multiple Agents

For robustness and diversification, you can deploy multiple agents from the same training:

```elixir
# Start a fleet of trading agents
{:ok, agent_ids} = Bardo.Examples.Applications.AlgoTrading.LiveAgent.start_agent_fleet(
  :my_trading_experiment,  # Experiment ID
  Bardo.Examples.Applications.AlgoTrading.Brokers.MetaTrader,  # Broker module
  [                        # List of broker configurations
    %{symbol: "EURUSD", timeframe: 15, account_id: "12345678"},
    %{symbol: "GBPUSD", timeframe: 15, account_id: "12345678"},
    %{symbol: "USDJPY", timeframe: 15, account_id: "12345678"}
  ],
  [
    nodes: [:node1@host, :node2@host],  # Distribute agents across nodes
    adaptation_enabled: true             # Enable continuous learning
  ]
)

# Get performance reports from all agents
fleet_performance = Bardo.Examples.Applications.AlgoTrading.LiveAgent.get_fleet_performance(agent_ids)
```

## Continuous Learning

One of the most powerful features of the Bardo trading system is continuous learning. This allows agents to adapt to changing market conditions after deployment.

### How Continuous Learning Works

1. The agent collects and stores its trading experience
2. Periodically, it adjusts its neural network based on recent performance
3. Successful trading patterns are reinforced
4. Unsuccessful patterns are modified

This creates an agent that can adapt to changing market conditions and continue improving its strategy after deployment.

### Enabling Continuous Learning

```elixir
# Enable continuous learning for an agent
Bardo.Examples.Applications.AlgoTrading.LiveAgent.enable_continuous_learning(
  agent_id,
  0.01,  # Learning rate (how quickly the agent adapts)
  10     # Update interval (apply updates every 10 trades)
)
```

## Example Workflows

Here are some example workflows for common algorithmic trading tasks:

### Basic Training and Testing

```elixir
# Run a simple forex trading experiment
mix run_algo_trading --market forex --symbol EURUSD --timeframe 15 --generations 100 --population 100

# Test the best agent on out-of-sample data
mix run_algo_trading --test --test-period last_month
```

### Advanced Training with Substrate Encoding

```elixir
# Start an IEx session
iex -S mix

# Configure experiment with substrate encoding
config = %{
  market: :forex,
  symbol: "EURUSD",
  timeframe: 15,
  population_size: 100,
  generations: 100,
  use_substrate: true,  # Enable substrate encoding
  input_time_points: 60,
  input_price_levels: 20,
  input_data_types: 10
}

# Run the experiment
Bardo.Examples.Applications.AlgoTrading.run(:substrate_experiment, config)

# Test the best agent
Bardo.Examples.Applications.AlgoTrading.test_best_agent(:substrate_experiment)
```

### Distributed Training on Multiple Nodes

```elixir
# Start an Elixir node with a name
iex --name trainer@hostname -S mix

# Connect to other nodes
Node.connect(:'node1@hostname')
Node.connect(:'node2@hostname')
Node.connect(:'node3@hostname')

# Verify connections
Node.list()

# Start distributed training
{:ok, experiment_id} = Bardo.Examples.Applications.AlgoTrading.DistributedTraining.start_distributed_training(
  :distributed_experiment,
  %{
    market: :forex,
    symbol: "EURUSD",
    timeframe: 15,
    population_size: 500,
    generations: 200
  }
)

# Monitor progress
:timer.sleep(60000)  # Wait a minute
Bardo.Examples.Applications.AlgoTrading.DistributedTraining.get_training_status(experiment_id)
```

### Live Trading Deployment

```elixir
# Get the best agent from a completed experiment
{:ok, best_agent} = Bardo.Examples.Applications.AlgoTrading.DistributedTraining.get_best_agent(:distributed_experiment)

# Export agent for deployment
Bardo.Examples.Applications.AlgoTrading.LiveAgent.export_agents(:distributed_experiment, "agents.json", 5)

# Import agents on another system
{:ok, agents} = Bardo.Examples.Applications.AlgoTrading.LiveAgent.import_agents("agents.json")
best_agent = List.first(agents)

# Start a live trading agent
{:ok, agent_id} = Bardo.Examples.Applications.AlgoTrading.LiveAgent.start_link(
  :live_agent,
  best_agent,
  Bardo.Examples.Applications.AlgoTrading.Brokers.MetaTrader,
  %{
    symbol: "EURUSD",
    timeframe: 15,
    account_id: "12345678",
    api_key: "your_api_key",
    api_url: "http://localhost:5000"
  }
)

# Enable continuous learning
Bardo.Examples.Applications.AlgoTrading.LiveAgent.enable_continuous_learning(agent_id)

# Monitor performance
:timer.sleep(3600000)  # Wait an hour
Bardo.Examples.Applications.AlgoTrading.LiveAgent.get_status(agent_id)
```

## Advanced Topics

### Custom Market Simulators

You can create custom market simulators by implementing the `PrivateScape` behavior:

```elixir
defmodule MyCustomMarketSimulator do
  @behaviour Bardo.AgentManager.PrivateScape
  
  # Implement required callbacks
  @impl Bardo.AgentManager.PrivateScape
  def init(params) do
    # Initialize your simulator
  end
  
  @impl Bardo.AgentManager.PrivateScape
  def sense(params, state) do
    # Process sensor requests
  end
  
  @impl Bardo.AgentManager.PrivateScape
  def actuate(function, params, agent_id, state) do
    # Process actuator requests
  end
  
  @impl Bardo.AgentManager.PrivateScape
  def terminate(reason, state) do
    # Clean up resources
  end
end
```

### Custom Broker Interfaces

To connect to different trading platforms, implement the `BrokerInterface` behavior:

```elixir
defmodule MyCustomBroker do
  @behaviour Bardo.Examples.Applications.AlgoTrading.Brokers.BrokerInterface
  
  # Implement required callbacks for the broker interface
  @impl Bardo.Examples.Applications.AlgoTrading.Brokers.BrokerInterface
  def connect(params) do
    # Connect to the broker
  end
  
  @impl Bardo.Examples.Applications.AlgoTrading.Brokers.BrokerInterface
  def disconnect(params) do
    # Disconnect from the broker
  end
  
  # Implement other required callbacks...
end
```

### Custom Fitness Functions

You can create custom fitness functions to optimize for specific trading objectives:

```elixir
# Sharpe ratio optimization
def sharpe_ratio_fitness(trading_results) do
  profit_loss = Map.get(trading_results, :profit_loss, 0.0)
  drawdown = Map.get(trading_results, :max_drawdown, 100.0)
  win_rate = Map.get(trading_results, :win_rate, 0.0)
  sharpe = Map.get(trading_results, :sharpe_ratio, 0.0)
  
  # Prioritize Sharpe ratio but also consider other metrics
  [
    sharpe * 100,             # Primary objective: risk-adjusted return
    profit_loss,              # Secondary objective: absolute return
    -drawdown * 5,            # Penalize drawdown
    win_rate * 50             # Reward consistency
  ]
end
```

### Custom Neural Network Architectures

You can experiment with different neural network architectures by customizing the substrate encoding:

```elixir
# Create a deeper network with more hidden layers
genotype = Bardo.Examples.Applications.AlgoTrading.SubstrateEncoding.create_substrate_genotype(%{
  input_time_points: 60,
  input_price_levels: 20,
  input_data_types: 10,
  hidden_layers: 5,           # More hidden layers
  hidden_neurons_per_layer: 30, # More neurons per layer
  output_neurons: 5           # More outputs for finer control
})
```

### Ensemble Trading Strategies

You can implement ensemble strategies that combine multiple neural networks:

```elixir
# Start multiple agents with different strategies
{:ok, agent_ids} = Bardo.Examples.Applications.AlgoTrading.LiveAgent.start_agent_fleet(
  :ensemble_experiment,
  broker_module,
  [broker_config, broker_config, broker_config]  # Same config for all agents
)

# Create an ensemble strategy that aggregates their decisions
# This would be a custom module you implement
Bardo.Examples.Applications.AlgoTrading.Ensemble.start_ensemble(
  :ensemble_strategy,
  agent_ids,
  broker_module,
  broker_config,
  %{aggregation_method: :weighted_vote}
)
```

## Frequently Asked Questions

### Will the agent adapt to changing market conditions?

Yes, if you enable continuous learning. The agent will collect its trading experience and periodically adjust its neural network to improve performance.

### How much data is needed for training?

For most forex pairs, at least 1-2 years of historical data is recommended. More complex strategies may require more data.

### Can I use this for high-frequency trading?

The current implementation is designed for medium to low-frequency trading (minutes to hours). High-frequency trading would require custom optimizations.

### How do I handle broker-specific requirements?

Implement a custom broker interface that handles the specific requirements of your broker.

### Is this ready for production use?

This is primarily research and development software. Proper risk management and extensive testing are essential before deploying any trading system with real money.

## Conclusion

Bardo's algorithmic trading module provides a powerful platform for developing, training, and deploying neural network-based trading strategies. With features like substrate encoding, distributed training, and continuous learning, it offers advanced capabilities for tackling the challenges of financial markets.

Remember that trading involves significant risk. Always start with small positions, implement proper risk management, and continuously monitor your trading systems when deployed to live markets.
=== ./docs/quickstart.md ===
# Bardo Quickstart Guide

This guide will help you get started with Bardo for your neuroevolution tasks. We'll walk through basic setup, running included examples, and creating a simple XOR experiment.

## Installation

Add Bardo to your mix.exs dependencies:

```elixir
def deps do
  [
    {:bardo, "~> 0.1.0"}
  ]
end
```

Then fetch and compile:

```bash
mix deps.get
mix compile
```

## Your First Experiment: XOR

Let's create a simple experiment to evolve a neural network that can solve the XOR problem.

### 1. Create a new project

```bash
mix new xor_example
cd xor_example
```

### 2. Add Bardo to dependencies

Add Bardo to `mix.exs`:

```elixir
def deps do
  [
    {:bardo, "~> 0.1.0"}
  ]
end
```

### 3. Create an XOR experiment module

Create a file `lib/xor_experiment.ex`:

```elixir
defmodule XorExperiment do
  @moduledoc """
  A simple example demonstrating how to evolve a neural network to solve the XOR problem.
  """
  
  alias Bardo.ExperimentManager
  alias Bardo.AgentManager.Cortex
  
  def run do
    # Create a new experiment
    experiment_id = "xor_experiment_#{:os.system_time(:millisecond)}"
    
    # Configure the experiment
    config = %{
      # Population settings
      population_size: 100,
      max_generations: 100,
      species_distance_threshold: 0.5,
      
      # Neural network settings
      activation_function: :sigmoid,
      weight_range: {-1.0, 1.0},
      bias_range: {-1.0, 1.0},
      
      # Mutation settings
      mutation_rate: 0.3,
      add_neuron_probability: 0.1,
      add_link_probability: 0.2,
      
      # Evaluation settings
      fitness_goal: 3.9 # Perfect solution would be 4.0
    }
    
    IO.puts("Starting XOR experiment: #{experiment_id}")
    
    # Create and configure the experiment
    {:ok, _} = ExperimentManager.new_experiment(experiment_id)
    :ok = ExperimentManager.configure(experiment_id, config)
    
    # Start the experiment with XOR fitness function
    :ok = ExperimentManager.start_evaluation(experiment_id, &xor_fitness/1)
    
    # Wait for completion
    monitor_progress(experiment_id)
    
    # Test the best solution
    test_best_solution(experiment_id)
  end
  
  # Fitness function for XOR
  defp xor_fitness(genotype) do
    # Convert genotype to neural network
    nn = Cortex.from_genotype(genotype)
    
    # Define XOR test cases
    test_cases = [
      {[0.0, 0.0], [0.0]},
      {[0.0, 1.0], [1.0]},
      {[1.0, 0.0], [1.0]},
      {[1.0, 1.0], [0.0]}
    ]
    
    # Calculate error for each test case
    total_error = Enum.reduce(test_cases, 0.0, fn {inputs, expected}, acc ->
      # Run the neural network
      outputs = Cortex.activate(nn, inputs)
      
      # Calculate error (difference between expected and actual output)
      error = Enum.zip(outputs, expected)
              |> Enum.map(fn {output, target} -> abs(output - target) end)
              |> Enum.sum()
      
      # Add to total error
      acc + error
    end)
    
    # Convert error to fitness (lower error = higher fitness)
    4.0 - total_error
  end
  
  # Monitor experiment progress
  defp monitor_progress(experiment_id) do
    # Poll for status until complete
    case ExperimentManager.status(experiment_id) do
      {:completed, _} ->
        IO.puts("Experiment completed!")
      
      {:in_progress, %{generation: gen, best_fitness: fitness}} ->
        IO.puts("Generation: #{gen}, Best Fitness: #{fitness}")
        :timer.sleep(500)
        monitor_progress(experiment_id)
      
      {:error, reason} ->
        IO.puts("Error: #{inspect(reason)}")
    end
  end
  
  # Test the best solution against XOR test cases
  defp test_best_solution(experiment_id) do
    # Get best genotype
    {:ok, best_genotype} = ExperimentManager.get_best_solution(experiment_id)
    
    # Convert to neural network
    nn = Cortex.from_genotype(best_genotype)
    
    # Define test cases
    test_cases = [
      {[0.0, 0.0], [0.0]},
      {[0.0, 1.0], [1.0]},
      {[1.0, 0.0], [1.0]},
      {[1.0, 1.0], [0.0]}
    ]
    
    IO.puts("\nTesting best solution:")
    
    # Test each case
    Enum.each(test_cases, fn {inputs, expected} ->
      outputs = Cortex.activate(nn, inputs)
      
      input_str = inputs |> Enum.map(&Float.to_string/1) |> Enum.join(", ")
      output_str = outputs |> Enum.map(&Float.to_string/1) |> Enum.join(", ")
      expected_str = expected |> Enum.map(&Float.to_string/1) |> Enum.join(", ")
      
      IO.puts("Input: [#{input_str}] => Output: [#{output_str}] (Expected: [#{expected_str}])")
    end)
    
    # Display network topology
    IO.puts("\nNeural Network Structure:")
    IO.inspect(nn, label: "Neural Network")
  end
end
```

### 4. Run the experiment

```elixir
# In IEx
iex -S mix
iex> XorExperiment.run()
```

You should see output showing the progress of the evolutionary process, and finally the performance of the best neural network on the XOR problem.

## Running Built-in Examples

Bardo comes with several built-in examples you can run:

### Double Pole Balancing

```elixir
# Start IEx
iex -S mix

# Run double pole balancing without damping
iex> Bardo.Examples.Benchmarks.Dpb.run_without_damping()

# Run double pole balancing with damping
iex> Bardo.Examples.Benchmarks.Dpb.run_with_damping()
```

### Flatland Predator-Prey Simulation

```elixir
# Start IEx
iex -S mix

# Run the flatland simulation
iex> Bardo.Examples.Applications.Flatland.run()
```

## Next Steps

Now that you've run your first experiment, consider:

1. Exploring the [API documentation](api_reference.md) for details on all available functions
2. Checking out the [advanced guide](advanced.md) for more complex usage patterns
3. Looking at the source code of the included examples to understand more complex applications

For more information, refer to the [complete documentation](https://hexdocs.pm/bardo).
=== ./docs/postgres_db_adapter.md ===
# PostgreSQL Database Adapter for Bardo with Ecto

This document describes how to set up and use the PostgreSQL database adapter for Bardo, which provides persistent storage for experiments, populations, genotypes, and enables distributed training across multiple nodes using Ecto.

## Overview

The `Bardo.DBPostgres` module is a PostgreSQL-backed implementation of the `Bardo.DB` behavior using Ecto, designed for:

1. Persistent storage of neuroevolution experiments
2. Coordination between distributed nodes
3. Tracking and managing distributed training jobs
4. Automatic backups and migrations

This adapter is particularly useful when deploying Bardo on platforms like fly.io, where distributed training across multiple nodes can be leveraged effectively.

## Configuration

### Basic Configuration

Add the following to your configuration:

```elixir
# config/postgres.exs
import Config

# Configuration for Ecto repo
config :bardo, Bardo.Repo,
  url: System.get_env("DATABASE_URL") || "postgres://postgres:postgres@localhost:5432/bardo",
  ssl: String.to_existing_atom(System.get_env("DB_SSL") || "false"),
  pool_size: String.to_integer(System.get_env("POOL_SIZE") || "10"),
  queue_target: 5000,
  queue_interval: 1000

# Configuration for Postgres-backed database for Bardo
config :bardo, :db,
  adapter: Bardo.DBPostgres,
  auto_migrate: true,
  auto_backup: true,
  auto_register: true
```

### Loading the Configuration

Include the postgres.exs file in your config.exs:

```elixir
# config/config.exs
import_config "postgres.exs"
```

Or, for environment-specific configuration:

```elixir
# config/prod.exs
import_config "postgres.exs"
```

## Database Schema

The PostgreSQL adapter uses the following Ecto schemas:

1. `Bardo.Schemas.Experiment` - Stores experiment configurations and metadata
2. `Bardo.Schemas.Population` - Stores population data for each experiment
3. `Bardo.Schemas.Genotype` - Stores individual genotypes with their fitness values
4. `Bardo.Schemas.Result` - Stores evaluation results from experiments
5. `Bardo.Schemas.DistributedNode` - Tracks connected nodes and their status
6. `Bardo.Schemas.DistributedJob` - Manages distributed training jobs

## Migrations

The migrations are automatically run when the application starts (controlled by the `auto_migrate` config option). All necessary tables and indexes are created automatically.

## Distributed Training with fly.io

### Setting Up flyctl

1. Create a fly.toml file:

```toml
app = "bardo"
kill_signal = "SIGTERM"
kill_timeout = 60
processes = []

[env]
  # Adjust these for your deployment
  DATABASE_URL = "postgres://postgres:postgres@bardo-db.internal:5432/bardo"
  NODE_COOKIE = "your-secure-cookie-here"
  POOL_SIZE = "10"
  DB_SSL = "true"
  MIX_ENV = "prod"

# Connect to fly.io's private networking
[mounts]
  source = "bardo_data"
  destination = "/data"

[deploy]
  strategy = "canary"
```

2. Launch a PostgreSQL database on fly.io:

```bash
flyctl postgres create --name bardo-db
```

3. Deploy your Bardo application:

```bash
flyctl deploy
```

4. Scale to multiple instances for distributed training:

```bash
flyctl scale count 3
```

### Node Registration

Each Bardo node will automatically register itself with the database upon startup, allowing for coordination between nodes. You can check registered nodes with:

```elixir
{:ok, nodes} = Bardo.DBPostgres.list_nodes()
```

### Creating Distributed Jobs

To create a distributed training job:

```elixir
job_id = "training_job_#{:os.system_time(:millisecond)}"
job_config = %{
  experiment_id: "forex_trading_experiment",
  population_size: 100,
  generations: 500,
  fitness_function: :sharpe_ratio,
  substrate_encoding: :time_price_indicator
}

:ok = Bardo.DBPostgres.create_job(job_id, job_config)
```

### Monitoring Jobs

You can monitor job status:

```elixir
{:ok, jobs} = Bardo.DBPostgres.list_jobs(:running)
```

Or get details of a specific job:

```elixir
{:ok, job_info} = Bardo.DBPostgres.get_job_info("job_123")
```

## Example: Using the PostgreSQL Adapter

Here's a complete example of how to use the PostgreSQL adapter:

```elixir
# Configure the application to use PostgreSQL
Application.put_env(:bardo, Bardo.Repo, [
  url: "postgres://postgres:postgres@localhost:5432/bardo",
  pool_size: 10
])

Application.put_env(:bardo, :db, [
  adapter: Bardo.DBPostgres
])

# Start the application
{:ok, _} = Application.ensure_all_started(:bardo)

# Create an experiment
experiment = %{
  id: "experiment_1",
  name: "Forex Trading Experiment",
  description: "Testing NEAT algorithm on forex data",
  config: %{
    population_size: 100,
    generations: 50,
    fitness_function: :sharpe_ratio
  }
}

:ok = Bardo.DBPostgres.store(:experiment, "experiment_1", experiment)

# Create a population
population = %{
  id: "population_1",
  experiment_id: "experiment_1",
  name: "Initial Population",
  generation: 0,
  config: %{
    selection_algorithm: :tournament,
    tournament_size: 3
  }
}

:ok = Bardo.DBPostgres.store(:population, "population_1", population)

# Create a genotype
genotype = %{
  id: "genotype_1",
  population_id: "population_1",
  data: %{
    neurons: [...],
    connections: [...]
  },
  fitness: 0.75,
  fitness_details: %{
    sharpe_ratio: 0.75,
    max_drawdown: 0.15,
    total_return: 0.25
  }
}

:ok = Bardo.DBPostgres.store(:genotype, "genotype_1", genotype)

# Create a distributed job
:ok = Bardo.DBPostgres.create_job("job_1", %{
  experiment_id: "experiment_1",
  population_size: 100,
  generations: 50
})

# List all experiments
{:ok, experiments} = Bardo.DBPostgres.list(:experiment)

# Get a specific experiment
experiment = Bardo.DBPostgres.read("experiment_1", :experiment)
```

## Backups and Migrations

### Manual Backups

```elixir
{:ok, backup_file} = Bardo.DBPostgres.backup("/path/to/backups")
```

### Restoring from Backup

```elixir
:ok = Bardo.DBPostgres.restore("/path/to/backups/bardo_backup_2023-01-01.sql")
```

## Error Handling

The adapter includes comprehensive error handling for database operations:

- Connection errors are logged and returned as `{:error, reason}`
- Failed queries are caught and logged with detailed error information
- Automatic reconnection is attempted for temporary connection issues

## Advantages of Using Ecto

1. **Schema Validation**: Ecto changesets validate data before insertion
2. **Query Composability**: Ecto queries can be composed and reused
3. **Migrations**: Automatic database migrations for schema changes
4. **Associations**: Easily navigate between related records
5. **Type Safety**: Ecto provides type conversion and validation
6. **Transactions**: Wrap multiple operations in a transaction

## Performance Considerations

- Use appropriate `pool_size` based on your workload (default: 10)
- For write-heavy workloads, increase `pool_size` and consider sharding
- For large datasets, use pagination when reading results
- Monitor performance with:
  - Postgres metrics (connection count, query times)
  - Application metrics (queue times, request throughput)
=== ./docs/api_reference.md ===
# Bardo API Reference

This document provides details on the main public interfaces of Bardo, a neuroevolution library for Elixir.

## Bardo

The main module provides basic library information.

```elixir
Bardo.version()                 # Returns the current library version
```

## ExperimentManager

The `Bardo.ExperimentManager` module handles the creation and management of evolutionary experiments.

### Creating and Configuring Experiments

```elixir
# Create a new experiment
{:ok, _} = Bardo.ExperimentManager.new_experiment(experiment_id)

# Configure an experiment with specific parameters
:ok = Bardo.ExperimentManager.configure(experiment_id, %{
  population_size: 100,
  max_generations: 50,
  mutation_rate: 0.3
})

# Start evaluation using a fitness function
:ok = Bardo.ExperimentManager.start_evaluation(experiment_id, &my_fitness_function/1)

# Get the current status of an experiment
{:in_progress, stats} = Bardo.ExperimentManager.status(experiment_id)

# Get the best solution from an experiment
{:ok, best_genotype} = Bardo.ExperimentManager.get_best_solution(experiment_id)

# Stop an experiment
:ok = Bardo.ExperimentManager.stop(experiment_id)
```

### Common Configuration Parameters

| Parameter | Type | Description | Default |
|-----------|------|-------------|---------|
| `population_size` | integer | Number of agents in the population | 50 |
| `max_generations` | integer | Maximum number of generations | 100 |
| `mutation_rate` | float | Probability of mutation for each gene | 0.3 |
| `add_neuron_probability` | float | Probability of adding a neuron | 0.1 |
| `add_link_probability` | float | Probability of adding a link | 0.2 |
| `weight_range` | {float, float} | Min/max values for weight initialization | {-1.0, 1.0} |
| `fitness_goal` | float | Target fitness to end evolution | nil (run to max_generations) |
| `species_distance_threshold` | float | Threshold for speciation | 0.5 |
| `activation_function` | atom | Default activation function | :sigmoid |
| `evaluation_method` | atom | :generational or :steady_state | :generational |

## AgentManager

The `Bardo.AgentManager` module manages neural network agents.

### Creating and Working with Neural Networks

```elixir
# Create a neural network from a genotype
nn = Bardo.AgentManager.Cortex.from_genotype(genotype)

# Activate a neural network with inputs
output = Bardo.AgentManager.Cortex.activate(nn, inputs)

# Add a sensor to a neural network
nn = Bardo.AgentManager.Cortex.add_sensor(nn, sensor_module, params)

# Add an actuator to a neural network
nn = Bardo.AgentManager.Cortex.add_actuator(nn, actuator_module, params)
```

### Neuron Management

```elixir
# Add a neuron to a neural network
nn = Bardo.AgentManager.Cortex.add_neuron(nn, :hidden, %{
  activation_function: :sigmoid,
  bias: 0.0
})

# Connect neurons
nn = Bardo.AgentManager.Cortex.connect_neurons(nn, source_id, target_id, weight)

# Set neuron parameters
nn = Bardo.AgentManager.Cortex.set_neuron_params(nn, neuron_id, %{
  bias: 1.0,
  activation_function: :tanh
})
```

## PopulationManager

The `Bardo.PopulationManager` module manages populations of evolving agents.

### Genotype Management

```elixir
# Create a new genotype
genotype = Bardo.PopulationManager.Genotype.new()

# Add a neuron to a genotype
genotype = Bardo.PopulationManager.Genotype.add_neuron(genotype, :hidden)

# Add a connection to a genotype
genotype = Bardo.PopulationManager.Genotype.add_connection(
  genotype, 
  source_id, 
  target_id, 
  weight
)
```

### Mutation Operations

```elixir
# Mutate a genotype
mutated_genotype = Bardo.PopulationManager.GenomeMutator.mutate(genotype, %{
  add_neuron_probability: 0.1,
  add_link_probability: 0.2,
  mutate_weights_probability: 0.8
})

# Apply specific mutation
genotype = Bardo.PopulationManager.GenomeMutator.mutate_weights(genotype)
genotype = Bardo.PopulationManager.GenomeMutator.add_neuron(genotype)
genotype = Bardo.PopulationManager.GenomeMutator.add_link(genotype)
```

### Selection Algorithms

```elixir
# Select agents for reproduction
selected = Bardo.PopulationManager.SelectionAlgorithm.select(
  population, 
  selection_method, 
  selection_params
)
```

## ScapeManager

The `Bardo.ScapeManager` module manages environments that agents interact with.

```elixir
# Create a new scape
{:ok, scape_id} = Bardo.ScapeManager.new_scape(scape_type, params)

# Enter an agent into a scape
:ok = Bardo.ScapeManager.enter(scape_id, agent_id, params)

# Sense from the environment
{:ok, sensory_data} = Bardo.ScapeManager.sense(scape_id, agent_id, sensor_params)

# Act on the environment
:ok = Bardo.ScapeManager.actuate(scape_id, agent_id, actuator_params)

# Leave a scape
:ok = Bardo.ScapeManager.leave(scape_id, agent_id)
```

## Creating Custom Components

### Custom Sensor

```elixir
defmodule MySensor do
  @behaviour Bardo.AgentManager.Sensor
  
  @impl true
  def init(params) do
    # Initialize sensor state
    {:ok, params}
  end
  
  @impl true
  def sense(state, environment) do
    # Process environment to produce sensory signals
    sensory_data = process_environment(environment)
    
    # Return sensory data and updated state
    {:ok, sensory_data, state}
  end
  
  defp process_environment(environment) do
    # Custom logic to extract sensory information
    # ...
    [0.5, 0.2, 0.7]  # Example return value
  end
end
```

### Custom Actuator

```elixir
defmodule MyActuator do
  @behaviour Bardo.AgentManager.Actuator
  
  @impl true
  def init(params) do
    # Initialize actuator state
    {:ok, params}
  end
  
  @impl true
  def actuate(state, environment, output_vector) do
    # Process neural network output to affect environment
    new_environment = apply_outputs(environment, output_vector)
    
    # Return updated environment and state
    {:ok, new_environment, state}
  end
  
  defp apply_outputs(environment, output_vector) do
    # Custom logic to apply neural outputs to environment
    # ...
    updated_environment
  end
end
```

### Custom Fitness Function

```elixir
def my_fitness_function(genotype) do
  # Convert genotype to neural network
  nn = Bardo.AgentManager.Cortex.from_genotype(genotype)
  
  # Evaluate performance on some task
  performance = evaluate_performance(nn)
  
  # Return fitness score (higher is better)
  performance
end

defp evaluate_performance(nn) do
  # Custom evaluation logic
  # ...
  fitness_score
end
```

## Substrate Encoding

Bardo supports several types of substrate encoding for neural networks:

```elixir
# Configure hypercube substrate encoding
Bardo.ExperimentManager.configure(experiment_id, %{
  substrate: %{
    type: :hypercube,
    dimensions: 3,
    resolution: 5
  }
})

# Configure hyperplane substrate encoding
Bardo.ExperimentManager.configure(experiment_id, %{
  substrate: %{
    type: :hyperplane,
    input_dimensions: 2,
    output_dimensions: 1,
    hidden_layers: 1
  }
})
```

## Utility Functions

```elixir
# Save a genotype to file
Bardo.Utils.save_genotype(genotype, "models/best_genotype.gen")

# Load a genotype from file
loaded_genotype = Bardo.Utils.load_genotype("models/best_genotype.gen")

# Analyze neural network complexity
stats = Bardo.Utils.analyze_network(nn)

# Calculate compatibility distance between genotypes
distance = Bardo.PopulationManager.SpecieIdentifier.compatibility_distance(
  genotype1, 
  genotype2
)
```

## Event Handling

```elixir
# Subscribe to experiment events
Bardo.EventManager.subscribe(experiment_id, :generation_complete)

# Handle events
def handle_info({:generation_complete, experiment_id, stats}, state) do
  # Process generation statistics
  # ...
  {:noreply, state}
end
```

## Visualization

```elixir
# Generate visualization data
viz_data = Bardo.Visualization.generate_network_visualization(nn)

# Plot fitness over generations
Bardo.Visualization.plot_fitness(experiment_id)

# Plot species over generations
Bardo.Visualization.plot_species(experiment_id)

# Plot complexity over generations
Bardo.Visualization.plot_complexity(experiment_id)
```
=== ./docs/distributed_trading.md ===
# Distributed Training and Trading with Bardo

## Introduction

This guide explains how to set up and use Bardo's distributed capabilities for both training neural networks and deploying trading agents across multiple nodes. Leveraging Elixir's built-in distribution, Bardo provides a robust framework for parallelizing computation and ensuring fault tolerance.

## Table of Contents

1. [Setting Up Distributed Nodes](#setting-up-distributed-nodes)
2. [Distributed Training](#distributed-training)
3. [Distributed Trading](#distributed-trading)
4. [Fault Tolerance and Recovery](#fault-tolerance-and-recovery)
5. [Monitoring and Management](#monitoring-and-management)
6. [Example Configurations](#example-configurations)

## Setting Up Distributed Nodes

Before using Bardo's distributed capabilities, you need to set up your Elixir nodes to communicate with each other.

### 1. Starting Nodes

Start each Elixir node with a name:

```bash
# Primary node
iex --name primary@hostname -S mix

# Worker nodes
iex --name worker1@hostname -S mix
iex --name worker2@hostname -S mix
iex --name worker3@hostname -S mix
```

Replace `hostname` with the actual hostname or IP address of the machine.

### 2. Connecting Nodes

From the primary node, connect to all worker nodes:

```elixir
# Connect to worker nodes
Node.connect(:'worker1@hostname')
Node.connect(:'worker2@hostname')
Node.connect(:'worker3@hostname')

# Verify connections
Node.list()  # Should show all connected nodes
```

### 3. Cookie Security

Ensure all nodes share the same cookie for authentication:

```bash
# Set the same cookie for all nodes
elixir --name node@hostname --cookie mycookie -S mix
```

Or set the cookie in your `.erlang.cookie` file.

## Distributed Training

Bardo's distributed training uses an island-based approach where subpopulations evolve independently and occasionally share individuals.

### Key Concepts

1. **Islands**: Subpopulations that evolve independently
2. **Migration**: Periodic exchange of individuals between islands
3. **Node Assignment**: Distribution of islands across physical nodes
4. **Coordination**: Central coordination of the training process

### Starting Distributed Training

```elixir
alias Bardo.Examples.Applications.AlgoTrading.DistributedTraining

# Configure experiment
experiment_config = %{
  market: :forex,
  symbol: "EURUSD",
  timeframe: 15,
  population_size: 600,  # Will be divided among islands
  generations: 200
  # ... other configuration options
}

# Distribution options
distribution_options = [
  nodes: [Node.self() | Node.list()],  # Use all available nodes
  islands: 6,                           # Number of subpopulations
  migration_interval: 10,               # Migrate every 10 generations
  migration_rate: 0.1                   # Migrate 10% of population
]

# Start distributed training
{:ok, experiment_id} = DistributedTraining.start_distributed_training(
  :my_trading_experiment,
  experiment_config,
  distribution_options
)
```

### Monitoring Training Progress

```elixir
# Get current status
status = DistributedTraining.get_training_status(experiment_id)

# Display status information
IO.puts("Experiment: #{status.experiment_id}")
IO.puts("Status: #{status.status}")
IO.puts("Generation: #{status.generation}")
IO.puts("Elapsed time: #{status.elapsed_time} seconds")

# Check status of individual islands
Enum.each(status.islands_status, fn island ->
  IO.puts("Island #{island.island}: Node #{island.node}, Generation #{island.generation}")
end)
```

### Getting Results

```elixir
# Get the best agent from all islands
{:ok, best_agent} = DistributedTraining.get_best_agent(experiment_id)

# Display fitness metrics
IO.inspect(best_agent.fitness)

# Save the agent for later use
Bardo.Models.store(:best_agents, experiment_id, best_agent)
```

### Early Stopping

```elixir
# Stop training before completion if needed
DistributedTraining.stop_distributed_training(experiment_id)
```

## Distributed Trading

After training, you can deploy trading agents across multiple nodes for fault tolerance and diversification.

### Deploying a Single Agent

```elixir
alias Bardo.Examples.Applications.AlgoTrading.LiveAgent

# Get the best agent from training
{:ok, best_agent} = DistributedTraining.get_best_agent(experiment_id)

# Deploy on a specific node
node = :'trading_node@hostname'

# Define a remote function to start the agent
remote_fun = fn ->
  LiveAgent.start_link(
    :trading_agent,
    best_agent,
    broker_module,
    broker_config,
    options
  )
end

# Execute on remote node
:rpc.call(node, Kernel, :apply, [remote_fun, []])
```

### Deploying Multiple Agents (Fleet)

```elixir
# Configure multiple broker connections
broker_configs = [
  %{symbol: "EURUSD", timeframe: 15, account_id: "12345678"},
  %{symbol: "GBPUSD", timeframe: 15, account_id: "12345678"},
  %{symbol: "USDJPY", timeframe: 15, account_id: "12345678"}
]

# Nodes for deployment
nodes = [:'trading1@hostname', :'trading2@hostname', :'trading3@hostname']

# Start a fleet of agents
{:ok, agent_ids} = LiveAgent.start_agent_fleet(
  experiment_id,
  broker_module,
  broker_configs,
  [nodes: nodes, adaptation_enabled: true]
)
```

### Managing Agent Fleet

```elixir
# Get performance metrics from all agents
fleet_performance = LiveAgent.get_fleet_performance(agent_ids)

# Display metrics for each agent
Enum.each(fleet_performance, fn {agent_id, status} ->
  IO.puts("Agent: #{agent_id}")
  IO.puts("Position: #{status.position.direction}")
  IO.puts("P/L: $#{status.performance.total_profit - status.performance.total_loss}")
  IO.puts("Win rate: #{status.performance.win_rate * 100}%")
end)

# Update risk parameters for all agents
Enum.each(agent_ids, fn agent_id -> 
  LiveAgent.update_risk_params(agent_id, %{risk_per_trade: 0.005})
end)

# Stop all agents
Enum.each(agent_ids, fn agent_id ->
  LiveAgent.close_all_positions(agent_id)
  LiveAgent.stop_agent(agent_id)
end)
```

## Fault Tolerance and Recovery

Bardo's distributed system includes mechanisms for handling node failures gracefully.

### Island Migration on Node Failure

During distributed training, if a node fails:

1. The coordinator detects the failure
2. The affected islands are migrated to available nodes
3. Training continues with minimal interruption

```elixir
# Configure auto-recovery
distribution_options = [
  # ... other options
  auto_recovery: true
]
```

### Agent Failover

For live trading, implement agent failover:

```elixir
# Monitor node status regularly
def monitor_nodes(agent_ids) do
  # Check each node
  Enum.each(agent_ids, fn agent_id ->
    node = find_agent_node(agent_id)
    
    # If node is down, restart agent on another node
    unless Node.ping(node) == :pong do
      # Get state from persistent storage
      {:ok, agent_state} = get_agent_state(agent_id)
      
      # Find a healthy node
      new_node = Enum.find(Node.list(), fn n -> Node.ping(n) == :pong end)
      
      # Restart agent on new node
      restart_agent_on_node(new_node, agent_id, agent_state)
    end
  end)
  
  # Schedule next check
  Process.send_after(self(), :check_nodes, 60_000)
end
```

## Monitoring and Management

### Web Dashboard

Create a simple web dashboard using Phoenix to monitor distributed system:

```elixir
# In your Phoenix controller
def index(conn, _params) do
  # Get all running experiments
  experiments = list_running_experiments()
  
  # Get all deployed agents
  agents = list_deployed_agents()
  
  # Get node status
  nodes = [Node.self() | Node.list()]
  node_status = Enum.map(nodes, fn node -> {node, Node.ping(node)} end)
  
  render(conn, "dashboard.html", 
    experiments: experiments, 
    agents: agents,
    node_status: node_status
  )
end
```

### Remote Management Console

Use a dedicated management node:

```elixir
# On management node
defmodule BardoManager do
  def start_experiment(name, config, nodes) do
    # Start distributed training
    DistributedTraining.start_distributed_training(name, config, [nodes: nodes])
  end
  
  def deploy_agent(experiment_id, broker_config, node) do
    # Get best agent
    {:ok, agent} = DistributedTraining.get_best_agent(experiment_id)
    
    # Deploy to specified node
    # ... deployment code
  end
  
  def status_report do
    # Generate comprehensive status report
    # ... report generation code
  end
end
```

## Example Configurations

### Multi-Node Training Cluster

```elixir
# Configuration for a 4-node training cluster
nodes = [
  :'primary@192.168.1.100',
  :'worker1@192.168.1.101',
  :'worker2@192.168.1.102',
  :'worker3@192.168.1.103'
]

# Configure with 8 islands (2 per node)
distribution_options = [
  nodes: nodes,
  islands: 8,
  migration_interval: 10,
  migration_rate: 0.1,
  
  # Island specialization
  island_configs: [
    # Island 0: Exploration focused
    %{mutation_rate: 0.2, tournament_size: 3},
    
    # Island 1: Standard parameters
    %{mutation_rate: 0.1, tournament_size: 5},
    
    # Island 2: Exploitation focused
    %{mutation_rate: 0.05, tournament_size: 7},
    
    # Island 3: Substrate encoding
    %{use_substrate: true},
    
    # Islands 4-7: Variations
    %{mutation_rate: 0.15, tournament_size: 4},
    %{mutation_rate: 0.1, tournament_size: 5},
    %{mutation_rate: 0.05, tournament_size: 6, use_substrate: true},
    %{mutation_rate: 0.1, tournament_size: 5, use_substrate: true}
  ]
]
```

### Multi-Currency Trading Fleet

```elixir
# Configuration for trading multiple currency pairs
broker_configs = [
  # Major pairs
  %{symbol: "EURUSD", timeframe: 15, account_id: "12345678"},
  %{symbol: "GBPUSD", timeframe: 15, account_id: "12345678"},
  %{symbol: "USDJPY", timeframe: 15, account_id: "12345678"},
  %{symbol: "AUDUSD", timeframe: 15, account_id: "12345678"},
  
  # Minor pairs
  %{symbol: "EURGBP", timeframe: 15, account_id: "12345678"},
  %{symbol: "EURJPY", timeframe: 15, account_id: "12345678"},
  %{symbol: "GBPJPY", timeframe: 15, account_id: "12345678"},
  
  # Same pairs with different timeframes
  %{symbol: "EURUSD", timeframe: 5, account_id: "12345678"},
  %{symbol: "EURUSD", timeframe: 60, account_id: "12345678"}
]

# Distribute across 3 trading nodes
trading_nodes = [
  :'trading1@hostname',
  :'trading2@hostname',
  :'trading3@hostname'
]

# Deploy with various configurations
{:ok, agent_ids} = LiveAgent.start_agent_fleet(
  experiment_id,
  broker_module,
  broker_configs,
  [
    nodes: trading_nodes,
    adaptation_enabled: true,
    risk_params: %{
      risk_per_trade: 0.01,
      max_drawdown: 0.10
    }
  ]
)
```

## Advanced: Continuous Training Pipeline

For advanced users, you can create a continuous training and deployment pipeline:

```elixir
# Schedule regular retraining
defmodule ContinuousTraining do
  use GenServer
  
  def start_link(opts) do
    GenServer.start_link(__MODULE__, opts, name: :continuous_trainer)
  end
  
  def init(opts) do
    # Schedule first training
    schedule_training()
    {:ok, opts}
  end
  
  def handle_info(:train, state) do
    # Start new training
    experiment_id = :"retraining_#{DateTime.utc_now() |> DateTime.to_unix()}"
    
    # Start distributed training
    {:ok, _} = DistributedTraining.start_distributed_training(
      experiment_id,
      state.config,
      state.distribution_options
    )
    
    # Store experiment ID for later
    new_state = Map.put(state, :current_experiment, experiment_id)
    
    # Schedule completion check
    Process.send_after(self(), :check_completion, 60_000)
    
    {:noreply, new_state}
  end
  
  def handle_info(:check_completion, state) do
    # Check if current experiment is complete
    status = DistributedTraining.get_training_status(state.current_experiment)
    
    if status.status == :complete do
      # Deploy new agents
      deploy_new_agents(state.current_experiment)
      
      # Schedule next training
      schedule_training()
    else
      # Check again later
      Process.send_after(self(), :check_completion, 60_000)
    end
    
    {:noreply, state}
  end
  
  defp schedule_training do
    # Schedule next training in 1 week
    Process.send_after(self(), :train, 7 * 24 * 60 * 60 * 1000)
  end
  
  defp deploy_new_agents(experiment_id) do
    # Get existing agents
    existing_agents = list_deployed_agents()
    
    # Get new agent
    {:ok, new_agent} = DistributedTraining.get_best_agent(experiment_id)
    
    # Compare performance
    if better_performance?(new_agent, existing_agents) do
      # Replace existing agents with new one
      replace_agents(existing_agents, new_agent)
    end
  end
end
```

## Conclusion

Bardo's distributed capabilities provide a powerful framework for both training and deploying algorithmic trading systems at scale. By leveraging Elixir's built-in distribution and fault tolerance, you can create robust trading systems that evolve and adapt in real-world environments.

For detailed configuration examples, see the [example configurations](example_configs/distributed_training.exs) directory.
=== ./docs/substrate_encoding.md ===
# Substrate Encoding for Neural Networks in Algorithmic Trading

## Introduction

Substrate encoding is a powerful neural network encoding technique that provides significant advantages for trading applications. This document explains how substrate encoding works in Bardo's algorithmic trading framework and how to use it effectively.

## What is Substrate Encoding?

Substrate encoding (also known as Hypercube-based Neuroevolution of Augmenting Topologies or HyperNEAT) is a technique that encodes neural networks with a geometric interpretation of the problem domain. Rather than directly evolving connection weights in a fixed topology, substrate encoding evolves patterns of connectivity between neurons placed in a geometric space.

## Why Use Substrate Encoding for Trading?

Traditional neural networks for trading face several challenges:

1. **Time series complexity**: Market data contains complex temporal patterns
2. **Multi-scale patterns**: Important patterns exist at different timeframes
3. **Sparse relationships**: Only certain combinations of inputs matter
4. **Overfitting**: It's easy to fit to noise rather than signal

Substrate encoding helps address these challenges by:

1. **Geometric interpretation**: Maps market data to a coordinate space
2. **Regular patterns**: Captures geometric regularities in data
3. **Efficient representation**: Compresses complex patterns into fewer parameters
4. **Built-in regularization**: Provides natural constraints on model complexity

## How Substrate Encoding Works in Bardo

In Bardo's algorithmic trading system, substrate encoding maps market data into a 3D coordinate space:

- **X-axis (time)**: From recent to older candles
- **Y-axis (price)**: From high to low price levels
- **Z-axis (data type)**: Different types of data (OHLC, volume, indicators)

![Substrate Encoding](../erlang/docs/substrate_encoding.png)

### Key Components

1. **Input Encoding**: Market data is converted into a 3D grid
2. **Neuron Placement**: Neurons are placed at specific coordinates
3. **Connectivity Patterns**: Connections are established based on geometric rules
4. **Weight Assignment**: Connection weights are derived from patterns

## Implementation Details

### 1. Creating a Substrate-Encoded Genotype

The first step is to create a genotype that uses substrate encoding:

```elixir
genotype = Bardo.Examples.Applications.AlgoTrading.SubstrateEncoding.create_substrate_genotype(%{
  input_time_points: 60,    # 60 time periods of data
  input_price_levels: 20,   # 20 price levels
  input_data_types: 10,     # 10 different data types
  hidden_layers: 2,         # 2 hidden layers
  hidden_neurons_per_layer: 20, # 20 neurons per hidden layer
  output_neurons: 3         # 3 outputs (direction, size, risk)
})
```

This function:
- Creates input neurons arranged in a 3D grid
- Adds hidden layers with neurons in geometric patterns
- Adds output neurons at appropriate coordinates
- Establishes initial connectivity based on geometric rules

### 2. Converting Market Data to Substrate Format

When processing market data, you need to convert it to the substrate format:

```elixir
grid = Bardo.Examples.Applications.AlgoTrading.SubstrateEncoding.convert_price_data_to_substrate(
  price_data,    # List of price candles
  indicators,    # Map of technical indicators
  60, 20, 10     # Dimensions matching the genotype
)
```

This function:
- Maps price data into a 3D grid representation
- Places different data types along the Z-axis
- Uses activation functions to represent price levels
- Normalizes all values to appropriate ranges

### 3. Activating the Neural Network

Before activating the neural network, the grid needs to be flattened to match the input neurons:

```elixir
inputs = Bardo.Examples.Applications.AlgoTrading.SubstrateEncoding.flatten_substrate_grid(
  grid, genotype
)

# Activate the neural network with the inputs
{:ok, outputs} = Bardo.AgentManager.Cortex.activate(cortex, inputs)
```

The flattening process:
- Maps each point in the 3D grid to the corresponding input neuron
- Matches coordinates in the grid to substrate coordinates of neurons
- Creates a map of neuron IDs to input values

## Evolutionary Process with Substrate Encoding

When evolving substrate-encoded networks:

1. **Initialization**: Start with a population of substrate-encoded networks
2. **Evaluation**: Test networks on the trading simulator
3. **Selection**: Select the best-performing networks
4. **Mutation**: Modify connectivity patterns and weights
5. **Reproduction**: Create the next generation
6. **Repeat**: Continue until convergence or generation limit

The key advantage is that mutations affect connectivity patterns rather than individual weights, allowing for more effective exploration of the solution space.

## Practical Example: Forex Trading

Here's a complete example of using substrate encoding for forex trading:

```elixir
# Configure experiment with substrate encoding
config = %{
  market: :forex,
  symbol: "EURUSD",
  timeframe: 15,
  population_size: 100,
  generations: 100,
  use_substrate: true,  # Enable substrate encoding
  input_time_points: 60,
  input_price_levels: 20,
  input_data_types: 10
}

# Run the experiment
Bardo.Examples.Applications.AlgoTrading.run(:substrate_experiment, config)

# Test the best agent
Bardo.Examples.Applications.AlgoTrading.test_best_agent(:substrate_experiment)

# Get the best agent for deployment
{:ok, best_agent} = Bardo.Examples.Applications.AlgoTrading.DistributedTraining.get_best_agent(:substrate_experiment)

# Deploy to live trading
{:ok, agent_id} = Bardo.Examples.Applications.AlgoTrading.LiveAgent.start_link(
  :live_agent,
  best_agent,
  broker_module,
  broker_config,
  [substrate_encoding: true]  # Important to use substrate encoding for live data too
)
```

## Advanced Customization

You can customize substrate encoding for specific trading applications:

### Custom Coordinate Mappings

```elixir
# Use a different coordinate system
custom_mapping = fn price_data, indicators, dims ->
  # Custom mapping logic...
  grid
end

# Use the custom mapping in your configuration
config = %{
  # ...
  use_substrate: true,
  substrate_mapping: custom_mapping
}
```

### Different Activation Functions

```elixir
# Create substrate with different activation functions
genotype = Bardo.Examples.Applications.AlgoTrading.SubstrateEncoding.create_substrate_genotype(%{
  # ...
  activation_functions: %{
    input: :tanh,
    hidden: :relu,
    output: :sigmoid
  }
})
```

### Irregular Neuron Distributions

```elixir
# Create substrate with custom neuron placement
genotype = Bardo.Examples.Applications.AlgoTrading.SubstrateEncoding.create_substrate_genotype(%{
  # ...
  neuron_placement: :concentrated,  # More neurons in recent timeframes
  concentration_factor: 2.0         # How concentrated neurons are
})
```

## Performance Considerations

Substrate encoding has some implications for performance:

- **Training time**: Typically requires more generations than direct encoding
- **Evaluation efficiency**: Faster evaluation due to more regular structure
- **Memory usage**: Can be higher due to the geometric representation
- **Runtime performance**: Usually comparable to standard neural networks

For best results:

1. Use distributed training for larger population sizes
2. Start with smaller networks and gradually increase complexity
3. Tune mutation rates for substrate-specific operations
4. Consider population diversity to avoid premature convergence

## Conclusion

Substrate encoding is a powerful technique for algorithmic trading that leverages geometric patterns in market data. By using this approach, you can create trading systems that are more robust, adaptable, and capable of capturing complex market dynamics.

For more information, see the [Algorithmic Trading Guide](algo_trading_guide.md) and [Example Configurations](example_configs/substrate_forex.exs).
=== ./docs/example_configs/substrate_forex.exs ===
# Substrate-encoded Forex Trading Configuration
# Use this configuration with: Bardo.Examples.Applications.AlgoTrading.run(:my_experiment, config)

alias Bardo.Examples.Applications.AlgoTrading.SubstrateEncoding

# Create a substrate-encoded genotype initializer function
substrate_initializer = fn ->
  SubstrateEncoding.create_substrate_genotype(%{
    input_time_points: 60,     # 60 time points (candles)
    input_price_levels: 20,    # 20 price levels from high to low
    input_data_types: 10,      # 10 different data types (OHLC, indicators, etc.)
    hidden_layers: 3,          # 3 hidden layers
    hidden_neurons_per_layer: 25, # 25 neurons per hidden layer
    output_neurons: 3          # 3 outputs (direction, size, risk)
  })
end

# Create a substrate converter function
substrate_converter = fn price_data, indicators, genotype ->
  # Convert market data to substrate representation
  grid = SubstrateEncoding.convert_price_data_to_substrate(
    price_data, indicators, 60, 20, 10
  )
  
  # Flatten to neuron inputs
  SubstrateEncoding.flatten_substrate_grid(grid, genotype)
end

# Configuration for EURUSD trading with substrate encoding
config = %{
  # Basic experiment settings
  market: :forex,
  symbol: "EURUSD",
  timeframe: 15,               # 15-minute candles
  population_size: 150,        # 150 individuals in population
  generations: 200,            # Run for 200 generations
  data_window: 10000,          # Use 10,000 candles for training
  
  # Substrate encoding
  use_substrate: true,         # Enable substrate encoding
  genotype_initializer: substrate_initializer,
  population_converter: substrate_converter,
  
  # Evolution parameters
  mutation_rate: 0.1,          # Base mutation rate
  mutation_operators: [
    {:mutate_weights, :gaussian, 0.4},  # 40% chance of weight mutation
    {:add_neuron, 0.1},                 # 10% chance to add a neuron
    {:add_connection, 0.2},             # 20% chance to add a connection
    {:remove_connection, 0.05},         # 5% chance to remove a connection
    {:remove_neuron, 0.03}              # 3% chance to remove a neuron
  ],
  selection_algorithm: "TournamentSelectionAlgorithm",
  tournament_size: 5,          # Tournament size for selection
  elite_fraction: 0.1,         # Keep top 10% unchanged
  
  # Evaluation parameters
  fitness_function: :sharpe_ratio, # Optimize for risk-adjusted returns
  evaluation_params: %{
    initial_balance: 10000,    # Start with $10,000
    max_drawdown: 30,          # Maximum 30% drawdown allowed
    leverage: 50,              # 50:1 leverage
    commission: 0.0001,        # 0.01% commission per trade
    slippage: 2,               # 2 pips slippage
    spread: 2                  # 2 pips spread
  },
  
  # Training data options
  use_external_data: false,    # Use internal data
  test_period: "last_month"    # Test on the last month of data
}
=== ./docs/example_configs/live_agent.exs ===
# Live Trading Agent Configuration
# Use this with: Bardo.Examples.Applications.AlgoTrading.LiveAgent functions

alias Bardo.Examples.Applications.AlgoTrading.LiveAgent
alias Bardo.Examples.Applications.AlgoTrading.Brokers.MetaTrader
alias Bardo.Examples.Applications.AlgoTrading.DistributedTraining

# Step 1: Get the best agent from a completed experiment
experiment_id = :my_forex_experiment

# Step 2: Configure the broker connection
broker_module = MetaTrader  # MetaTrader broker module

broker_config = %{
  # Connection details
  api_url: "http://localhost:5000",  # MT REST API URL
  api_key: "YOUR_API_KEY",           # API key if required
  account_id: "12345678",            # MT account number
  
  # Trading parameters
  symbol: "EURUSD",                  # Trading symbol
  timeframe: 15,                     # Timeframe in minutes
  
  # Additional broker options
  execution_mode: :market,           # Market or limit orders
  demo_mode: true                    # Use demo account
}

# Step 3: Configure risk management
risk_params = %{
  risk_per_trade: 0.01,              # Risk 1% per trade
  max_drawdown: 0.10,                # Maximum 10% drawdown
  stop_loss: 0.02,                   # 2% stop loss
  take_profit: 0.04,                 # 4% take profit
  max_positions: 1,                  # Maximum 1 simultaneous position
  position_sizing: :fixed_risk,      # Position sizing method
  trailing_stop: true,               # Use trailing stops
  trailing_stop_distance: 0.01       # 1% trailing stop distance
}

# Step 4: Configure agent options
agent_options = [
  risk_params: risk_params,
  substrate_encoding: true,          # Use substrate encoding
  adaptation_enabled: false          # Start without continuous learning
]

# Step 5: Start the live trading agent
# To execute, remove the # from the lines below and fill in your actual agent ID
# Agent ID should be unique to avoid conflicts
#agent_id = :my_live_forex_agent

# Get the best agent from the completed experiment
#{:ok, best_agent} = DistributedTraining.get_best_agent(experiment_id)

# Start the live trading agent
#{:ok, _pid} = LiveAgent.start_link(agent_id, best_agent, broker_module, broker_config, agent_options)

# Step 6: Monitor the agent's performance
# LiveAgent.get_status(agent_id)

# Step 7: Enable continuous learning after some time
# LiveAgent.enable_continuous_learning(agent_id, 0.01, 10)

# Step 8: Update risk parameters if needed
#LiveAgent.update_risk_params(agent_id, %{
#  risk_per_trade: 0.005,            # Reduce risk to 0.5%
#  max_drawdown: 0.05                # Reduce max drawdown to 5%
#})

# Step 9: Stop the agent when done
# LiveAgent.close_all_positions(agent_id)
# Process.exit(Process.whereis(agent_id), :normal)

# Alternative: Deploy a fleet of agents
#broker_configs = [
#  %{symbol: "EURUSD", timeframe: 15, account_id: "12345678", api_url: "http://localhost:5000"},
#  %{symbol: "GBPUSD", timeframe: 15, account_id: "12345678", api_url: "http://localhost:5000"},
#  %{symbol: "USDJPY", timeframe: 15, account_id: "12345678", api_url: "http://localhost:5000"}
#]

#fleet_options = [
#  nodes: [Node.self(), :'node1@hostname', :'node2@hostname'],
#  adaptation_enabled: true
#]

#{:ok, agent_ids} = LiveAgent.start_agent_fleet(experiment_id, broker_module, broker_configs, fleet_options)

# Monitor the fleet
#LiveAgent.get_fleet_performance(agent_ids)
=== ./docs/example_configs/distributed_training.exs ===
# Distributed Training Configuration
# Use this with: Bardo.Examples.Applications.AlgoTrading.DistributedTraining.start_distributed_training/3

alias Bardo.Examples.Applications.AlgoTrading.DistributedTraining

# Step 1: Start the appropriate Elixir nodes
# On primary node: iex --name primary@hostname -S mix
# On worker nodes: iex --name worker1@hostname -S mix
#                  iex --name worker2@hostname -S mix
#                  iex --name worker3@hostname -S mix

# Step 2: Connect the nodes
# On primary node:
# Node.connect(:'worker1@hostname')
# Node.connect(:'worker2@hostname')
# Node.connect(:'worker3@hostname')

# Step 3: Configure the experiment
experiment_id = :distributed_forex_experiment

# Base configuration for the experiment
experiment_config = %{
  # Market settings
  market: :forex,
  symbol: "EURUSD",
  timeframe: 15,               # 15-minute candles
  
  # Population settings
  population_size: 600,        # 600 individuals (will be divided among islands)
  generations: 200,            # Run for 200 generations
  data_window: 10000,          # Use 10,000 candles for training
  
  # Evolution parameters
  mutation_rate: 0.1,          # Base mutation rate
  mutation_operators: [
    {:mutate_weights, :gaussian, 0.4},  # 40% chance of weight mutation
    {:add_neuron, 0.1},                 # 10% chance to add a neuron
    {:add_connection, 0.2},             # 20% chance to add a connection
    {:remove_connection, 0.05},         # 5% chance to remove a connection
    {:remove_neuron, 0.03}              # 3% chance to remove a neuron
  ],
  selection_algorithm: "TournamentSelectionAlgorithm",
  tournament_size: 5,          # Tournament size for selection
  elite_fraction: 0.1,         # Keep top 10% unchanged
  
  # Evaluation parameters
  fitness_function: :sharpe_ratio, # Optimize for risk-adjusted returns
  
  # Optional: Use substrate encoding on some islands
  use_substrate: true,         # Enable substrate encoding
  
  # Testing
  test_period: "last_month"    # Test on the last month of data
}

# Distribution options
distribution_options = [
  # List of nodes to use
  nodes: [
    :'primary@hostname',  # This node
    :'worker1@hostname',  # Additional worker nodes
    :'worker2@hostname',
    :'worker3@hostname'
  ],
  
  # Number of islands (population subgroups)
  islands: 6,  # Can be different from number of nodes
  
  # Migration frequency (in generations)
  migration_interval: 10,
  
  # Percent of population to migrate between islands
  migration_rate: 0.1,
  
  # Fault tolerance: automatically migrate islands if a node fails
  auto_recovery: true,
  
  # Custom island configurations
  island_configs: [
    # Island 0: High exploration
    %{
      mutation_rate: 0.2,         # Higher mutation rate
      tournament_size: 3,         # Lower selection pressure
      use_substrate: true         # Use substrate encoding
    },
    
    # Island 1: Standard parameters
    %{
      mutation_rate: 0.1,
      tournament_size: 5,
      use_substrate: true
    },
    
    # Island 2: High exploitation
    %{
      mutation_rate: 0.05,        # Lower mutation rate
      tournament_size: 7,         # Higher selection pressure
      elite_fraction: 0.2,        # More elitism
      use_substrate: true
    },
    
    # Island 3: High exploration, no substrate
    %{
      mutation_rate: 0.2,
      tournament_size: 3,
      use_substrate: false
    },
    
    # Island 4: Standard parameters, no substrate
    %{
      mutation_rate: 0.1,
      tournament_size: 5,
      use_substrate: false
    },
    
    # Island 5: High exploitation, no substrate
    %{
      mutation_rate: 0.05,
      tournament_size: 7,
      elite_fraction: 0.2,
      use_substrate: false
    }
  ]
]

# Step 4: Start distributed training
#{:ok, experiment_id} = DistributedTraining.start_distributed_training(
#  experiment_id,
#  experiment_config,
#  distribution_options
#)

# Step 5: Monitor progress
# :timer.sleep(60000)  # Wait a minute
# DistributedTraining.get_training_status(experiment_id)

# Step 6: Get results when finished
# {:ok, best_agent} = DistributedTraining.get_best_agent(experiment_id)

# Step 7: Stop training early if needed
# DistributedTraining.stop_distributed_training(experiment_id)
=== ./docs/flatland_tutorial.md ===
# Flatland Tutorial: Predator-Prey Co-evolution

This tutorial will walk you through setting up and running the Flatland simulation, one of the most fascinating examples included with Bardo. In this simulation, predator and prey agents co-evolve in a 2D world, developing increasingly sophisticated survival strategies.

## Overview

The Flatland simulation creates a 2D environment with:

- **Predators**: Red agents that hunt and consume prey
- **Prey**: Blue agents that try to avoid predators while eating plants
- **Plants**: Green resources that provide energy to prey

What makes this simulation particularly interesting is that both predator and prey agents are controlled by neural networks that evolve over time through natural selection.

## Running the Basic Simulation

### Step 1: Start an Elixir session

First, open an interactive Elixir session:

```bash
cd /path/to/bardo
iex -S mix
```

### Step 2: Run the default Flatland simulation

```elixir
iex> Bardo.Examples.Applications.Flatland.run()
```

This will start the simulation with default parameters. You'll see output describing the initialization process, followed by periodic updates on the evolutionary progress.

## Customizing the Simulation

You can customize the simulation with different parameters:

```elixir
iex> Bardo.Examples.Applications.Flatland.run(%{
  predator_population_size: 15,  # Default: 10
  prey_population_size: 30,      # Default: 20
  plant_quantity: 50,            # Default: 40
  world_size: {1200, 1200},      # Default: {1000, 1000}
  max_generations: 200,          # Default: 100
  steady_state: true,            # Use steady-state evolution (ongoing births/deaths)
  visualization: true            # Enable visualization (if available)
})
```

## Understanding the Agents

### Prey Agents

Prey agents are equipped with:

1. **Distance sensors**: Detect objects at different angles from the agent
2. **Color sensors**: Identify what type of object is detected (predator, prey, or plant)
3. **Two-wheel actuators**: Control movement direction and speed

Their neural networks evolve to accomplish these goals:
- Avoid predators
- Find and consume plants
- Manage energy efficiently

### Predator Agents

Predator agents have similar sensory and motor capabilities:

1. **Distance sensors**: Detect objects at different angles
2. **Color sensors**: Identify prey and distinguish from other predators
3. **Two-wheel actuators**: Control movement

Their neural networks evolve to:
- Track and catch prey
- Develop hunting strategies
- Manage energy efficiently

## Analyzing the Results

As the simulation runs, you'll observe several metrics:

### 1. Fitness Scores

```elixir
iex> Bardo.Examples.Applications.Flatland.plot_fitness("experiment_id")
```

This shows how average fitness for both species changes over time. Typically:
- Initially, scores are low as agents move randomly
- As agents learn to find food, scores increase
- When predators learn to hunt, prey fitness drops while predator fitness rises
- Eventually, an arms race develops as prey evolve evasion tactics

### 2. Neural Network Complexity

```elixir
iex> Bardo.Examples.Applications.Flatland.plot_complexity("experiment_id")
```

This shows how neural network size changes over time:
- Networks typically grow more complex as they evolve more sophisticated behaviors
- Predators and prey often develop different levels of complexity

### 3. Population Diversity

```elixir
iex> Bardo.Examples.Applications.Flatland.plot_diversity("experiment_id")
```

This shows genetic diversity within each population:
- Higher diversity means more exploration of possible solutions
- Lower diversity indicates convergence on successful strategies

### 4. Population Turnover

```elixir
iex> Bardo.Examples.Applications.Flatland.plot_turnover("experiment_id")
```

This shows death rates for each species:
- Higher prey turnover indicates successful predator hunting
- Lower predator turnover indicates successful feeding

## Interesting Behaviors to Watch For

As the simulation runs, several fascinating emergent behaviors may develop:

1. **Predator Ambush Tactics**: Predators may learn to wait near plant concentrations to ambush prey
2. **Prey Grouping**: Prey may develop swarming behaviors for protection
3. **Resource Management**: Both species tend to develop energy-efficient movement patterns
4. **Pattern Recognition**: Agents may learn to predict the behavior of other agents

## Extending the Simulation

You can extend the Flatland simulation in several ways:

### 1. Customizing Sensor Configurations

```elixir
# Create a custom sensor configuration
sensor_config = %{
  num_distance_sensors: 8,      # Default: 6
  sensor_range: 200,            # Default: 150
  sensor_resolution: "high"     # Default: "medium"
}

# Run with custom sensors
Bardo.Examples.Applications.Flatland.run(%{sensor_config: sensor_config})
```

### 2. Adding Environmental Features

You can modify the `Flatland` module to add features like:
- Obstacles
- Resource-rich regions
- Environmental hazards
- Seasonal changes

### 3. Creating Custom Agents

You can define new types of agents by creating modules that implement the sensor and actuator behaviors.

## Conclusion

The Flatland simulation demonstrates the power of neuroevolution to produce complex, adaptive behaviors through a process similar to natural selection. As you run the simulation, you'll observe the fascinating co-evolution of predator and prey species, each developing increasingly sophisticated strategies in response to the other.

For more details on the implementation, explore the following files:
- `lib/bardo/examples/applications/flatland.ex`
- `lib/bardo/examples/applications/flatland/flatland_sensor.ex`
- `lib/bardo/examples/applications/flatland/flatland_actuator.ex`
=== ./docs/advanced.md ===
# Advanced Bardo Usage

This guide covers advanced topics and techniques for working with Bardo.

## Custom Fitness Functions

A fitness function evaluates how well a neural network performs at a given task. Here's how to create your own:

```elixir
defmodule MyApp.FitnessFunctions do
  def xor_fitness(genotype) do
    # Create neural network from genotype
    nn = Bardo.AgentManager.Cortex.from_genotype(genotype)
    
    # Test cases for XOR
    test_cases = [
      {[0.0, 0.0], [0.0]},
      {[0.0, 1.0], [1.0]},
      {[1.0, 0.0], [1.0]},
      {[1.0, 1.0], [0.0]}
    ]
    
    # Calculate error across all test cases
    total_error = Enum.reduce(test_cases, 0, fn {inputs, expected}, acc ->
      outputs = Bardo.AgentManager.Cortex.activate(nn, inputs)
      error = Enum.sum(Enum.map(Enum.zip(outputs, expected), fn {o, e} -> abs(o - e) end))
      acc + error
    end)
    
    # Convert error to fitness (lower error = higher fitness)
    fitness = 4 - total_error
    
    # Return fitness score
    fitness
  end
end
```

When creating fitness functions, consider:
- Normalization: Keep fitness values in a consistent range
- Granularity: Provide enough differentiation between solutions
- Guidance: Shape the fitness landscape to guide evolution toward desired behaviors

## Developing Custom Sensors

Sensors provide input to neural networks. To create a custom sensor:

```elixir
defmodule MyApp.Sensors.TemperatureSensor do
  @behaviour Bardo.AgentManager.Sensor
  
  @impl true
  def init(params) do
    # Initialize sensor with given parameters
    {:ok, params}
  end
  
  @impl true
  def sense(state, world_state) do
    # Extract temperature from world state
    temperature = world_state.temperature
    
    # Normalize temperature to range [0,1]
    normalized_temp = (temperature - state.min_temp) / (state.max_temp - state.min_temp)
    
    # Return sensor reading
    {:ok, [normalized_temp], state}
  end
end
```

When designing sensors:
- Normalize inputs to a standard range (typically [-1, 1] or [0, 1])
- Consider sensor placement and field of view
- Determine appropriate sensor resolution and update frequency

## Developing Custom Actuators

Actuators allow neural networks to interact with their environment:

```elixir
defmodule MyApp.Actuators.JointActuator do
  @behaviour Bardo.AgentManager.Actuator
  
  @impl true
  def init(params) do
    # Initialize actuator with given parameters
    {:ok, params}
  end
  
  @impl true
  def actuate(state, world_state, output_vector) do
    # Extract joint angle from neural network output
    [joint_angle] = output_vector
    
    # Scale from [-1,1] to actual joint limits
    scaled_angle = state.min_angle + (joint_angle + 1) * (state.max_angle - state.min_angle) / 2
    
    # Apply to world state
    new_world_state = put_in(world_state.joint_positions[state.joint_id], scaled_angle)
    
    # Return new world state
    {:ok, new_world_state, state}
  end
end
```

## Advanced Mutation Operators

Bardo comes with standard mutation operators, but you can create custom ones:

```elixir
defmodule MyApp.CustomMutator do
  def mutate_weights_with_noise(genotype, config) do
    noise_scale = config[:noise_scale] || 0.1
    
    # Apply Gaussian noise to all weights
    updated_weights = Enum.map(genotype.weights, fn {id, weight} ->
      noise = :rand.normal() * noise_scale
      {id, weight + noise}
    end)
    
    # Return updated genotype
    %{genotype | weights: Map.new(updated_weights)}
  end
end
```

To use custom mutation operators:

```elixir
# Configure experiment with custom mutation operator
Bardo.ExperimentManager.configure(experiment, %{
  population_size: 50,
  mutation_operators: [
    {MyApp.CustomMutator, :mutate_weights_with_noise, [%{noise_scale: 0.05}], 0.3}
  ]
})
```

## Distributed Training

Bardo can distribute evolutionary computations across multiple Erlang nodes:

```elixir
# Connect to other nodes
Node.connect(:"node1@192.168.1.101")
Node.connect(:"node2@192.168.1.102")

# Configure experiment with distributed settings
Bardo.ExperimentManager.configure(experiment, %{
  distributed: true,
  nodes: [node(), :"node1@192.168.1.101", :"node2@192.168.1.102"],
  population_per_node: 20
})
```

## Custom Neural Activation Functions

By default, Bardo uses sigmoid activation functions, but you can define others:

```elixir
defmodule MyApp.ActivationFunctions do
  def relu(x) do
    max(0, x)
  end
  
  def leaky_relu(x) do
    if x > 0, do: x, else: 0.01 * x
  end
  
  def tanh(x) do
    :math.tanh(x)
  end
end

# Use custom activation function when creating a neuron
Bardo.PopulationManager.Genotype.add_neuron(genotype, :hidden, %{
  activation_function: {MyApp.ActivationFunctions, :tanh}
})
```

## Custom Substrate Encodings

For complex problems, you may want to use indirect encodings (where genotype doesn't directly specify each connection):

```elixir
# Configure hypercube substrate encoding
Bardo.ExperimentManager.configure(experiment, %{
  substrate: %{
    type: :hypercube,
    dimensions: 3,
    resolution: 5,
    connectivity_function: {MyApp.Substrate, :connectivity_function}
  }
})
```

## Real-time Visualization

To visualize evolution progress:

```elixir
# Start visualization server
Bardo.Visualization.start(%{
  port: 8080,
  refresh_rate: 1000,  # ms
  metrics: [:fitness, :complexity, :diversity]
})

# Configure experiment to send data to visualization
Bardo.ExperimentManager.configure(experiment, %{
  visualize: true,
  visualization_endpoint: "http://localhost:8080/data"
})
```

## Saving and Loading Evolved Networks

To save your best evolved networks for later use:

```elixir
# Get best solution from experiment
best_solution = Bardo.ExperimentManager.get_best_solution(experiment)

# Save to file
Bardo.Utils.save_genotype(best_solution, "models/xor_solution.gen")

# Load from file
loaded_genotype = Bardo.Utils.load_genotype("models/xor_solution.gen")

# Create neural network from saved genotype
nn = Bardo.AgentManager.Cortex.from_genotype(loaded_genotype)
```

## Custom Selection Algorithms

Bardo supports different selection algorithms for choosing which individuals reproduce:

```elixir
defmodule MyApp.SelectionAlgorithms do
  def tournament_selection(population, tournament_size, elite_count) do
    # Sort population by fitness
    sorted_pop = Enum.sort_by(population, fn agent -> agent.fitness end, :desc)
    
    # Keep elite individuals
    {elite, rest} = Enum.split(sorted_pop, elite_count)
    
    # Fill remaining slots with tournament selection
    selected = elite ++ tournament_select(rest, length(sorted_pop) - elite_count, tournament_size)
    
    selected
  end
  
  defp tournament_select(population, count, tournament_size) do
    Enum.map(1..count, fn _ ->
      # Select random individuals for tournament
      contestants = Enum.take_random(population, tournament_size)
      
      # Return winner (highest fitness)
      Enum.max_by(contestants, fn agent -> agent.fitness end)
    end)
  end
end

# Configure experiment with custom selection algorithm
Bardo.ExperimentManager.configure(experiment, %{
  selection_algorithm: {MyApp.SelectionAlgorithms, :tournament_selection, [5, 2]}
})
```

## Speciation and Diversity Preservation

To maintain genetic diversity during evolution:

```elixir
# Configure experiment with speciation
Bardo.ExperimentManager.configure(experiment, %{
  enable_speciation: true,
  species_distance_threshold: 0.3,
  species_compatibility_function: {Bardo.PopulationManager.SpecieIdentifier, :compatibility_distance},
  species_elitism: true,
  minimum_species_size: 5
})
```
